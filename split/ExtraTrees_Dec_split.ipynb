{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74a2e03f-7f53-45d3-9945-3d9065f53457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, cross_val_predict, train_test_split\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import classification_report, make_scorer, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import optuna\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b007c3d0-065a-42ea-b03d-35774aac386d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook started at: 2025-01-21 16:05:45.267963\n"
     ]
    }
   ],
   "source": [
    "notebook_start_time = datetime.now()\n",
    "print(f\"Notebook started at: {notebook_start_time}\")\n",
    "df = pd.read_csv('complete_decimal_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c64901d7-c60e-4f16-b4d8-815a4f9c1260",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding target variable\n",
    "label_encoder = LabelEncoder()\n",
    "df['specific_class_encoded'] = label_encoder.fit_transform(df['specific_class'])\n",
    "\n",
    "#Preparing features and target\n",
    "X_full = df.drop(columns=['label', 'category', 'specific_class', 'specific_class_encoded'])\n",
    "y_full = df['specific_class_encoded']\n",
    "\n",
    "#Scaling features\n",
    "scaler = StandardScaler()\n",
    "X_full_scaled = scaler.fit_transform(X_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2094fdd-95e4-47fa-a117-e22f1496a37b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Training and testing sets Splits (80/20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_full_scaled, y_full, test_size=0.2, random_state=42)\n",
    "\n",
    "#Optimization function for ExtraTrees\n",
    "def optimize_extra_trees(trial):\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "    max_depth = trial.suggest_int('max_depth', 10, 50, log=True)\n",
    "    \n",
    "    clf = ExtraTreesClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scorer = make_scorer(f1_score, average='macro')\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=kfold, scoring=scorer, n_jobs=-1)\n",
    "\n",
    "    \n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "932b79af-9cab-4b1e-b1b1-6df10ede6630",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-21 16:05:47,030] A new study created in memory with name: no-name-17c71f4f-a5aa-425b-a39c-f93bd3696346\n",
      "[I 2025-01-21 16:12:09,153] Trial 6 finished with value: 0.9999485626547765 and parameters: {'n_estimators': 142, 'max_depth': 11}. Best is trial 6 with value: 0.9999485626547765.\n",
      "[I 2025-01-21 16:16:19,703] Trial 3 finished with value: 1.0 and parameters: {'n_estimators': 94, 'max_depth': 16}. Best is trial 3 with value: 1.0.\n",
      "[I 2025-01-21 16:24:40,230] Trial 1 finished with value: 0.9999485626547765 and parameters: {'n_estimators': 292, 'max_depth': 11}. Best is trial 3 with value: 1.0.\n",
      "[I 2025-01-21 16:26:48,680] Trial 4 finished with value: 1.0 and parameters: {'n_estimators': 184, 'max_depth': 45}. Best is trial 3 with value: 1.0.\n",
      "[I 2025-01-21 16:27:34,733] Trial 0 finished with value: 0.9848411367251817 and parameters: {'n_estimators': 75, 'max_depth': 10}. Best is trial 3 with value: 1.0.\n",
      "[I 2025-01-21 16:33:10,328] Trial 7 finished with value: 1.0 and parameters: {'n_estimators': 192, 'max_depth': 29}. Best is trial 3 with value: 1.0.\n",
      "[I 2025-01-21 16:33:58,342] Trial 2 finished with value: 1.0 and parameters: {'n_estimators': 178, 'max_depth': 44}. Best is trial 3 with value: 1.0.\n",
      "[I 2025-01-21 16:34:53,090] Trial 8 finished with value: 1.0 and parameters: {'n_estimators': 130, 'max_depth': 31}. Best is trial 3 with value: 1.0.\n",
      "[I 2025-01-21 16:34:58,953] Trial 9 finished with value: 1.0 and parameters: {'n_estimators': 106, 'max_depth': 19}. Best is trial 3 with value: 1.0.\n",
      "[I 2025-01-21 16:36:06,325] Trial 5 finished with value: 0.9999485626547765 and parameters: {'n_estimators': 244, 'max_depth': 11}. Best is trial 3 with value: 1.0.\n"
     ]
    }
   ],
   "source": [
    "#hyperparameters optimization using Optuna\n",
    "study_et = optuna.create_study(direction=\"maximize\")\n",
    "study_et.optimize(optimize_extra_trees, n_trials=10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5d99fff-1a90-4772-93ce-343860b50340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for ExtraTrees: {'n_estimators': 94, 'max_depth': 16}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best parameters for ExtraTrees: {study_et.best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c910b08c-8e81-468e-be05-dd81dc542934",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model with the best parameters from Optuna\n",
    "best_et = ExtraTreesClassifier(**study_et.best_params, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a48ac027-00e2-49bf-934f-e2eaa2e5bcf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training time: 46.53 seconds\n",
      "Classification Report on Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    244790\n",
      "           1       1.00      1.00      1.00     15021\n",
      "           2       1.00      1.00      1.00      1974\n",
      "           3       1.00      1.00      1.00     10902\n",
      "           4       1.00      1.00      1.00      4968\n",
      "           5       1.00      1.00      1.00      3989\n",
      "\n",
      "    accuracy                           1.00    281644\n",
      "   macro avg       1.00      1.00      1.00    281644\n",
      "weighted avg       1.00      1.00      1.00    281644\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "best_et.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "training_duration = end_time - start_time\n",
    "print(f\"Model training time: {training_duration:.2f} seconds\")\n",
    "\n",
    "y_pred = best_et.predict(X_test)\n",
    "\n",
    "#Classification report on test data\n",
    "print(\"Classification Report on Test Set:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b76dd289-f44f-4739-bda4-f1ebc5fc6ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook ended at: 2025-01-21 16:36:54.630344\n",
      "Total notebook runtime: 0:31:09.362381\n"
     ]
    }
   ],
   "source": [
    "# Log the end time\n",
    "notebook_end_time = datetime.now()\n",
    "print(f\"Notebook ended at: {notebook_end_time}\")\n",
    "\n",
    "# Calculate the total duration\n",
    "notebook_duration = notebook_end_time - notebook_start_time\n",
    "print(f\"Total notebook runtime: {notebook_duration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a2d1ca8-c921-4920-b34f-85114c27386c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 1.00, Test Accuracy: 1.00\n",
      "Training F1 Score: 1.00, Test F1 Score: 1.00\n",
      "Training and test performance are comparable.\n",
      "\n",
      "Classification Report (Training):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    978947\n",
      "           1       1.00      1.00      1.00     59642\n",
      "           2       1.00      1.00      1.00      8017\n",
      "           3       1.00      1.00      1.00     43998\n",
      "           4       1.00      1.00      1.00     19983\n",
      "           5       1.00      1.00      1.00     15988\n",
      "\n",
      "    accuracy                           1.00   1126575\n",
      "   macro avg       1.00      1.00      1.00   1126575\n",
      "weighted avg       1.00      1.00      1.00   1126575\n",
      "\n",
      "Classification Report (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    244790\n",
      "           1       1.00      1.00      1.00     15021\n",
      "           2       1.00      1.00      1.00      1974\n",
      "           3       1.00      1.00      1.00     10902\n",
      "           4       1.00      1.00      1.00      4968\n",
      "           5       1.00      1.00      1.00      3989\n",
      "\n",
      "    accuracy                           1.00    281644\n",
      "   macro avg       1.00      1.00      1.00    281644\n",
      "weighted avg       1.00      1.00      1.00    281644\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,accuracy_score, make_scorer, precision_score, recall_score, f1_score\n",
    "\n",
    "#Evaluate on training set\n",
    "y_train_pred = best_et.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_f1 = f1_score(y_train, y_train_pred, average=\"macro\")\n",
    "\n",
    "#Evaluate on test set\n",
    "y_test_pred = best_et.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred, average=\"macro\")\n",
    "\n",
    "\n",
    "#Compare results\n",
    "print(f\"Training Accuracy: {train_accuracy:.2f}, Test Accuracy: {test_accuracy:.2f}\")\n",
    "print(f\"Training F1 Score: {train_f1:.2f}, Test F1 Score: {test_f1:.2f}\")\n",
    "\n",
    "if train_accuracy > test_accuracy:\n",
    "    print(\"The model might be overfitting.\")\n",
    "elif train_accuracy < test_accuracy:\n",
    "    print(\"Test set performs better than training.\")\n",
    "else:\n",
    "    print(\"Training and test performance are comparable.\")\n",
    "\n",
    "#Classification reports\n",
    "print(\"\\nClassification Report (Training):\")\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "print(\"Classification Report (Test):\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e52cef2-ef5d-4bf3-9df0-5a131c06a7ea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
