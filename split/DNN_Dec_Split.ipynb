{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f4aeee0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input\n",
    "import optuna\n",
    "from datetime import datetime\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "381e131d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook started at: 2025-01-21 18:29:07.393839\n"
     ]
    }
   ],
   "source": [
    "notebook_start_time = datetime.now()\n",
    "print(f\"Notebook started at: {notebook_start_time}\")\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('complete_decimal_dataset.csv')\n",
    "\n",
    "# Encode target variable\n",
    "label_encoder = LabelEncoder()\n",
    "df['specific_class_encoded'] = label_encoder.fit_transform(df['specific_class'])\n",
    "\n",
    "# Prepare features and target\n",
    "X = df.drop(columns=['label', 'category', 'specific_class', 'specific_class_encoded'])\n",
    "y = df['specific_class_encoded']\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split for evaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, stratify=y, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2e840c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dnn_model(input_dim, layers, units, dropout_rate, learning_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(input_dim,)))\n",
    "    model.add(Dense(units, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    for _ in range(layers - 1):\n",
    "        model.add(Dense(units, activation='relu'))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(len(np.unique(y)), activation='softmax'))  # Output layer for multi-class classification\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69a73e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-21 18:29:09,666] A new study created in memory with name: no-name-97984fac-83be-4de2-b560-ea34eb5ec88d\n",
      "C:\\Users\\FX505\\Documents\\tese_env\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "[I 2025-01-21 19:30:26,500] Trial 7 finished with value: 0.9999920129776001 and parameters: {'layers': 3, 'units': 48, 'dropout_rate': 0.21452488962601535, 'learning_rate': 0.00012574537075716414, 'batch_size': 96, 'epochs': 10}. Best is trial 7 with value: 0.9999920129776001.\n",
      "[I 2025-01-21 19:57:01,174] Trial 1 finished with value: 0.9808321595191956 and parameters: {'layers': 3, 'units': 16, 'dropout_rate': 0.3945862731213424, 'learning_rate': 0.0034230410862882417, 'batch_size': 112, 'epochs': 18}. Best is trial 7 with value: 0.9999920129776001.\n",
      "[I 2025-01-21 20:22:05,453] Trial 4 finished with value: 0.9999920129776001 and parameters: {'layers': 4, 'units': 128, 'dropout_rate': 0.41467009471348093, 'learning_rate': 0.0015291829865857602, 'batch_size': 128, 'epochs': 24}. Best is trial 7 with value: 0.9999920129776001.\n",
      "[I 2025-01-21 20:22:36,738] Trial 6 finished with value: 0.999997341632843 and parameters: {'layers': 1, 'units': 80, 'dropout_rate': 0.18532205829849147, 'learning_rate': 0.00022542562810660247, 'batch_size': 112, 'epochs': 25}. Best is trial 6 with value: 0.999997341632843.\n",
      "[I 2025-01-21 20:24:31,689] Trial 9 finished with value: 0.999997341632843 and parameters: {'layers': 2, 'units': 112, 'dropout_rate': 0.383965488435004, 'learning_rate': 0.0005525168712053452, 'batch_size': 96, 'epochs': 21}. Best is trial 6 with value: 0.999997341632843.\n",
      "[I 2025-01-21 21:27:00,403] Trial 2 finished with value: 0.9999902367591857 and parameters: {'layers': 2, 'units': 80, 'dropout_rate': 0.4372800155453822, 'learning_rate': 0.00012784395722404223, 'batch_size': 64, 'epochs': 32}. Best is trial 6 with value: 0.999997341632843.\n",
      "[I 2025-01-21 21:33:35,993] Trial 0 finished with value: 1.0 and parameters: {'layers': 4, 'units': 128, 'dropout_rate': 0.2774088047541644, 'learning_rate': 0.0009501230980605523, 'batch_size': 112, 'epochs': 45}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-01-21 21:40:44,756] Trial 8 finished with value: 0.9999982237815856 and parameters: {'layers': 3, 'units': 96, 'dropout_rate': 0.3491637026251538, 'learning_rate': 0.0006702946152151761, 'batch_size': 48, 'epochs': 26}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-01-21 21:43:54,102] Trial 3 finished with value: 0.9999902367591857 and parameters: {'layers': 1, 'units': 80, 'dropout_rate': 0.4453815797926145, 'learning_rate': 0.0011017420733470495, 'batch_size': 48, 'epochs': 30}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-01-21 22:33:57,135] Trial 5 finished with value: 0.9999902367591857 and parameters: {'layers': 1, 'units': 80, 'dropout_rate': 0.3595446925437056, 'learning_rate': 0.0004812317667085973, 'batch_size': 16, 'epochs': 23}. Best is trial 0 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'layers': 4, 'units': 128, 'dropout_rate': 0.2774088047541644, 'learning_rate': 0.0009501230980605523, 'batch_size': 112, 'epochs': 45}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def optimize_dnn(trial):\n",
    "    # Hyperparameters to optimize\n",
    "    layers = trial.suggest_int('layers', 1, 5)\n",
    "    units = trial.suggest_int('units', 16, 128, step=16)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 128, step=16)\n",
    "    epochs = trial.suggest_int('epochs', 10, 50)\n",
    "    \n",
    "    # Create the model\n",
    "    model = create_dnn_model(X_train.shape[1], layers, units, dropout_rate, learning_rate)\n",
    "    \n",
    "    #Stratified K-Fold Cross-Validation\n",
    "    #Convert to NumPy arrays\n",
    "    X_train_np = np.array(X_train)  \n",
    "    y_train_np = np.array(y_train)  \n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    for train_idx, val_idx in skf.split(X_train_np, y_train_np):\n",
    "        X_ktrain, X_kval = X_train_np[train_idx], X_train_np[val_idx]\n",
    "        y_ktrain, y_kval = y_train_np[train_idx], y_train_np[val_idx]\n",
    "    \n",
    "        # Train the model \n",
    "        model.fit(X_ktrain, y_ktrain, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    \n",
    "        # Evaluate on validation data\n",
    "        _, accuracy = model.evaluate(X_kval, y_kval, verbose=0)\n",
    "        scores.append(accuracy)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "# Run the Optuna study\n",
    "study_dnn = optuna.create_study(direction=\"maximize\")\n",
    "study_dnn.optimize(optimize_dnn, n_trials=10, n_jobs=-1)\n",
    "\n",
    "# Best hyperparameters\n",
    "print(\"Best hyperparameters:\", study_dnn.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d174510",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FX505\\Documents\\tese_env\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/45\n",
      "\u001b[1m10059/10059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 2ms/step - accuracy: 0.9873 - loss: 0.0410\n",
      "Epoch 2/45\n",
      "\u001b[1m10059/10059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 5.1067e-04\n",
      "Epoch 3/45\n",
      "\u001b[1m10059/10059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 6.2611e-04\n",
      "Epoch 4/45\n",
      "\u001b[1m10059/10059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 6.3112e-04\n",
      "Epoch 5/45\n",
      "\u001b[1m10059/10059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 4.4573e-04\n",
      "Epoch 6/45\n",
      "\u001b[1m10059/10059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 3.4542e-04\n",
      "Epoch 7/45\n",
      "\u001b[1m10059/10059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 5.6483e-04\n",
      "Epoch 8/45\n",
      "\u001b[1m10059/10059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 5.1268e-04\n",
      "Epoch 9/45\n",
      "\u001b[1m10059/10059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 3.7351e-04\n",
      "Epoch 10/45\n",
      "\u001b[1m10059/10059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 3.8914e-04\n",
      "Epoch 11/45\n",
      "\u001b[1m10059/10059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 2.1975e-04\n",
      "Epoch 12/45\n",
      "\u001b[1m10059/10059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 6.4015e-04\n",
      "Epoch 13/45\n",
      "\u001b[1m10059/10059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 5.5741e-04\n",
      "Epoch 14/45\n",
      "\u001b[1m10059/10059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 5.8770e-04\n",
      "Epoch 15/45\n",
      "\u001b[1m10059/10059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 2.8677e-04\n",
      "Epoch 16/45\n",
      "\u001b[1m10059/10059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 3.6277e-04\n",
      "Epoch 17/45\n",
      "\u001b[1m10059/10059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 4.5259e-04\n",
      "Epoch 18/45\n",
      "\u001b[1m10059/10059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 7.3159e-04\n",
      "Epoch 19/45\n",
      "\u001b[1m10059/10059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 6.8232e-04\n",
      "Epoch 20/45\n",
      "\u001b[1m10059/10059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 3.7904e-04\n",
      "Epoch 21/45\n",
      "\u001b[1m10059/10059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 7.3153e-04\n",
      "Epoch 22/45\n",
      "\u001b[1m10059/10059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 3.2905e-04\n",
      "Epoch 23/45\n",
      "\u001b[1m10059/10059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 5.9162e-04\n",
      "Epoch 24/45\n",
      "\u001b[1m10059/10059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 3.8084e-04\n",
      "Epoch 25/45\n",
      "\u001b[1m10059/10059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 0.0017\n",
      "Epoch 26/45\n",
      "\u001b[1m10059/10059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 6.1276e-04\n",
      "Epoch 27/45\n",
      "\u001b[1m10059/10059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 6.6654e-04\n",
      "Epoch 28/45\n",
      "\u001b[1m10059/10059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 2.5938e-04\n",
      "Epoch 29/45\n",
      "\u001b[1m10059/10059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 5.0049e-04\n",
      "Epoch 30/45\n",
      "\u001b[1m10059/10059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 5.9875e-04\n",
      "Epoch 31/45\n",
      "\u001b[1m10059/10059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 2.5428e-04\n",
      "Epoch 32/45\n",
      "\u001b[1m10059/10059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 3.7591e-04\n",
      "Epoch 33/45\n",
      "\u001b[1m10059/10059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 3.6246e-04\n",
      "Epoch 34/45\n",
      "\u001b[1m10059/10059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 5.2233e-04\n",
      "Epoch 35/45\n",
      "\u001b[1m10059/10059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 4.4732e-04\n",
      "Epoch 36/45\n",
      "\u001b[1m10059/10059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 3.9889e-04\n",
      "Epoch 37/45\n",
      "\u001b[1m10059/10059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 6.5558e-04\n",
      "Epoch 38/45\n",
      "\u001b[1m10059/10059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 4.8004e-04\n",
      "Epoch 39/45\n",
      "\u001b[1m10059/10059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 8.6025e-04\n",
      "Epoch 40/45\n",
      "\u001b[1m10059/10059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 2.8818e-04\n",
      "Epoch 41/45\n",
      "\u001b[1m10059/10059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 6.1047e-04\n",
      "Epoch 42/45\n",
      "\u001b[1m10059/10059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 4.1012e-04\n",
      "Epoch 43/45\n",
      "\u001b[1m10059/10059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 4.8567e-04\n",
      "Epoch 44/45\n",
      "\u001b[1m10059/10059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 3.3740e-04\n",
      "Epoch 45/45\n",
      "\u001b[1m10059/10059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 4.2135e-04\n",
      "Model training time: 1021.06 seconds\n",
      "\u001b[1m8802/8802\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step\n",
      "Classification Report on Test Data:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    244748\n",
      "           1       1.00      1.00      1.00     14933\n",
      "           2       1.00      1.00      1.00      1998\n",
      "           3       1.00      1.00      1.00     10980\n",
      "           4       1.00      1.00      1.00      4990\n",
      "           5       1.00      1.00      1.00      3995\n",
      "\n",
      "    accuracy                           1.00    281644\n",
      "   macro avg       1.00      1.00      1.00    281644\n",
      "weighted avg       1.00      1.00      1.00    281644\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Extract best parameters\n",
    "best_params = study_dnn.best_params\n",
    "\n",
    "# Train the final model with the best parameters\n",
    "final_model = create_dnn_model(X_train.shape[1],\n",
    "                               layers=best_params['layers'],\n",
    "                               units=best_params['units'],\n",
    "                               dropout_rate=best_params['dropout_rate'],\n",
    "                               learning_rate=best_params['learning_rate'])\n",
    "\n",
    "start_time = time.time()\n",
    "# Train on full training set\n",
    "final_model.fit(X_train, y_train, epochs=best_params['epochs'], batch_size=best_params['batch_size'], verbose=1)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "training_duration = end_time - start_time\n",
    "print(f\"Model training time: {training_duration:.2f} seconds\")\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred = np.argmax(final_model.predict(X_test), axis=1)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report on Test Data:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3eb117c4-7c8c-40d5-9271-f0eb7ca8611d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook ended at: 2025-01-21 22:51:11.003978\n",
      "Total notebook runtime: 4:22:03.610139\n"
     ]
    }
   ],
   "source": [
    "# Log the end time\n",
    "notebook_end_time = datetime.now()\n",
    "print(f\"Notebook ended at: {notebook_end_time}\")\n",
    "\n",
    "# Calculate the total duration\n",
    "notebook_duration = notebook_end_time - notebook_start_time\n",
    "print(f\"Total notebook runtime: {notebook_duration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09bc4ec-556f-4c4d-8741-c880e33546c8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
